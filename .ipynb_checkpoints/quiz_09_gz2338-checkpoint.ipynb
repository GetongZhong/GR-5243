{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Quiz\n",
    "\n",
    "## Getong Zhong - gz2338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Replace the Name and UNI in cell above and the notebook name\n",
    "\n",
    "Replace all '____' below using the instructions provided.\n",
    "\n",
    "When completed, \n",
    " - make sure you've replaced Name and UNI in the first cell and filename (eg: quiz_09-hw2592)\n",
    " - Click the github link shared on canvas for quiz10 to generate your own repository of quiz10\n",
    " - Click your own repository of quiz10 to commit your answers to github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quiz, we're going to load documents from 2 categories (space, cars) in the 20newsgroups dataset.\n",
    "\n",
    "The goal is to train a classifier that classifies documents into these 2 categories based on a term frequency representation of the documents.\n",
    "\n",
    "We will then calculate mean cross-validaion accuracy of a RandomForestClassifier using this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977\n"
     ]
    }
   ],
   "source": [
    "# Import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset using fetch_20newsgroups().\n",
    "#   Only fetch the two categories of interest using categories=['sci.space','rec.autos']\n",
    "# Store in the result into newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=['sci.space', 'rec.autos'], shuffle=True, random_state=42)\n",
    "\n",
    "# Store the newsgroups.data as docs, newsgroups.target as y and newsgroups.target_names as y_names\n",
    "docs = newsgroups.data\n",
    "y = newsgroups.target\n",
    "y_names = newsgroups.target_names\n",
    "\n",
    "# Print the number of observations by printing the length of docs\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: hathaway@stsci.edu\n",
      "Subject: Re: Vandalizing the sky.\n",
      "Lines: 32\n",
      "Organization: Space Telescope Science Institute\n",
      "Distribution: na\n",
      "\n",
      "In article <C65LJ5.5Az@fs7.ece.cmu.edu>, loss@fs7.ECE.CMU.EDU (Doug Loss) writes:\n",
      ">    I didn't want to quote all the stuff that's been said recently, I\n",
      "> just wanted to add a point.\n",
      "> \n",
      ".. \n",
      "> then enforces those rights for them.  Here in the U.S., the constitution\n",
      "> provides a \"Bill of Rights\" from which most if not all legal rights are\n",
      "> considered to derive.  I'm sure that most other countries have\n",
      "\n",
      "These seem hardly like the groups to discuss this in, but HUH??? \n",
      "All legitimate power to enforce these rights derives from the consent \n",
      "of the governed, not from no steenkin' piece of paper.  Civilized gov'mnt \n",
      "is not an autonomous computer program, it's interactive.  The Constitution \n",
      "was made by the people and can be trashed by us - it ain't no sacred \n",
      "scripture from which rights flow.  Our 'rights' come from our souls. \n",
      "And I sure didn't see any request to vote on trashing the sky.  \n",
      "Again - my opinion only - we keep our rights by using them, not going to \n",
      "some court.  \n",
      "\n",
      "> comparable documents.  If you can persuade a court that you have a right\n",
      "> to a dark sky derived in some manner from the Bill of Rights (in the\n",
      "> U.S.), you can prevent (maybe) these billboards from being launched.  To\n",
      "> keep anyone in the world from launching then gets into international law\n",
      "> and the International Court of Justice (correct name?) in the Hague,\n",
      "> something I know little about.\n",
      "> \n",
      "> Doug Loss\n",
      "> loss@husky.bloomu.edu\n",
      "\n",
      "\n",
      "Most gracious regards, \n",
      "WHH \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'From: hathaway@stsci.edu\\nSubject: Re: Vandalizing the sky.\\nLines: 32\\nOrganization: Space Telescope Science Institute\\nDistribution: na\\n\\nIn article <C65LJ5.5Az@fs7.ece.cmu.edu>, loss@fs7.ECE.CMU.EDU (Doug Loss) writes:\\n>    I didn\\'t want to quote all the stuff that\\'s been said recently, I\\n> just wanted to add a point.\\n> \\n.. \\n> then enforces those rights for them.  Here in the U.S., the constitution\\n> provides a \"Bill of Rights\" from which most if not all legal rights are\\n> considered to derive.  I\\'m sure that most other countries have\\n\\nThese seem hardly like the groups to discuss this in, but HUH??? \\nAll legitimate power to enforce these rights derives from the consent \\nof the governed, not from no steenkin\\' piece of paper.  Civilized gov\\'mnt \\nis not an autonomous computer program, it\\'s interactive.  The Constitution \\nwas made by the people and can be trashed by us - it ain\\'t no sacred \\nscripture from which rights flow.  Our \\'rights\\' come from our souls. \\nAnd I sure didn\\'t see any request to vote on trashing the sky.  \\nAgain - my opinion only - we keep our rights by using them, not going to \\nsome court.  \\n\\n> comparable documents.  If you can persuade a court that you have a right\\n> to a dark sky derived in some manner from the Bill of Rights (in the\\n> U.S.), you can prevent (maybe) these billboards from being launched.  To\\n> keep anyone in the world from launching then gets into international law\\n> and the International Court of Justice (correct name?) in the Hague,\\n> something I know little about.\\n> \\n> Doug Loss\\n> loss@husky.bloomu.edu\\n\\n\\nMost gracious regards, \\nWHH \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the text of the first document in docs.\n",
    "# Note: try printing both with and without the print() statement\n",
    "#  with the print statement, linebreaks are parsed,\n",
    "#  without, linebreaks are printed as excape characters\n",
    "print(docs[0])\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Print the target value of the first document in y.\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "# Print the target_name of the first document using y_names and y.\n",
    "print(y_names[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer to Convert To TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1977, 9483)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a CountVectorizer object. It should\n",
    "#   lowercase all text, \n",
    "#   include both unigrams and bigrams\n",
    "#   exclude terms that occur in fewer than 10 documents\n",
    "#   exclude terms that occur in more than 95% of documents\n",
    "# Store as cvect\n",
    "cvect = CountVectorizer(lowercase=True, ngram_range=(1, 2), min_df=10, max_df=0.95)\n",
    "\n",
    "# Fit cvect on docs and transform docs into their term frequency representation.\n",
    "# Store as X_tf\n",
    "X_tf = cvect.fit_transform(docs)\n",
    "\n",
    "# Print the shape of X_tf. \n",
    "# The number of rows should match the number of documents \n",
    "#    and the number of columns should be in the thousands\n",
    "print(X_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zoo toronto', 'zoology', 'zoology between', 'zoology kipling', 'zoology lines']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Print out the last 5 terms in the learned vocabulary in cvect\n",
    "#   using .get_feature_names() which returns a list of terms corresponding\n",
    "#   to the order of the columns in X_tf\n",
    "# They should all be related to zoos\n",
    "feature_names = cvect.get_feature_names()\n",
    "print([term for term in feature_names if \"zoo\" in term][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['verne collector', 'team and', 'budget enviroment', 'gordon2 on', 'the gobs']\n"
     ]
    }
   ],
   "source": [
    "# The stopwords learned by cvect are stored as a set in cvect.stop_words_\n",
    "# We'd like to print out a small subset of these terms.\n",
    "# One way to get a subset of a set is to treat it as a list.\n",
    "# First, convert the stop_words_ set to a list.\n",
    "# Store as stop_words_list\n",
    "stop_words_list = list(cvect.stop_words_)\n",
    "\n",
    "# Print out the first 5 elements in stop_words_list.\n",
    "# Note that, since a set is unordered, \n",
    "#    there is no meaning to the ordering of these terms and they may vary over runs.\n",
    "print(stop_words_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean CV Accuracy Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9742079018028384\n"
     ]
    }
   ],
   "source": [
    "# Import cross_val_score from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Get a set of 5-fold CV scores using\n",
    "#    a RandomForestClassifier with 50 trees and n_jobs=-1\n",
    "#    and the full dataset X_tf and y\n",
    "# Store as cv_scores\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
    "cv_scores = cross_val_score(rf, X_tf, y, cv=5)\n",
    "# Print the mean of these cv_scores. The mean accuracy should be above .9\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer stores the feature names (terms in the vocabulary) in two ways:\n",
    "#  1. as a dictionary of term:colum_index pairs, accessed via cvect.vocabulary_\n",
    "#  2. as a list of terms, in column index order, accessed via cvect.get_feature_names()\n",
    "#\n",
    "# We can get the indices of the most important features by retraining a RandomForestClassifier on X_tf,y\n",
    "#  and accessing .feature_importances_\n",
    "#\n",
    "# Using some combination of the above data-structures, \n",
    "#  print out the top 10 terms in the vocabulary\n",
    "#  ranked by the feature importances learned by a RandomForestClassifier with 50 trees\n",
    "# \n",
    "# The terms you find will likely not be surprising given the document categories.\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
